"""
Aura Lead Hunter - Find Employers Mode
=======================================
Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð ÐÐ‘ÐžÐ¢ÐžÐ”ÐÐ¢Ð•Ð›Ð•Ð™ Ð² Ð°Ñ€Ð±Ð¸Ñ‚Ñ€Ð°Ð¶Ðµ:
- Ð¢Ð¸Ð¼Ð»Ð¸Ð´Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¸Ñ‰ÑƒÑ‚ Ð±Ð°Ð¹ÐµÑ€Ð¾Ð²
- Ð’Ð»Ð°Ð´ÐµÐ»ÑŒÑ†Ñ‹ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸ÑÐ¼Ð¸
- ÐÐ³ÐµÐ½Ñ‚ÑÑ‚Ð²Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ð°Ð±Ð¸Ñ€Ð°ÑŽÑ‚ Ð»ÑŽÐ´ÐµÐ¹

Usage:
    py find_employers.py                    # ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð½Ñ‹Ð¼ Ñ‡Ð°Ñ‚Ð°Ð¼
    py find_employers.py --chats chat1,chat2  # Ð¡Ð²Ð¾Ð¸ Ñ‡Ð°Ñ‚Ñ‹
"""

import asyncio
import argparse
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Set, Optional
from dataclasses import dataclass, field

from config.settings import settings
from core.telegram_client import TelegramClient
from core.rate_limiter import RateLimiter
from core.intent_analyzer import IntentAnalyzer, LeadAnalysis
from storage.csv_exporter import CSVExporter
from storage.report_generator import generate_html_report
from utils.logger import get_logger, ThoughtType

from telethon.tl.types import Message, User
from telethon.errors import FloodWaitError


# ===== ÐšÐ›Ð®Ð§Ð•Ð’Ð˜ÐšÐ˜ Ð”Ð›Ð¯ ÐŸÐžÐ˜Ð¡ÐšÐ Ð ÐÐ‘ÐžÐ¢ÐžÐ”ÐÐ¢Ð•Ð›Ð•Ð™ =====
EMPLOYER_KEYWORDS = [
    # ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð½Ð°Ð¹Ð¼ (RU)
    'Ð¸Ñ‰Ñƒ Ð±Ð°ÐµÑ€Ð°', 'Ð¸Ñ‰Ñƒ Ð±Ð°Ð¹ÐµÑ€Ð°', 'Ð½ÑƒÐ¶ÐµÐ½ Ð±Ð°ÐµÑ€', 'Ð½ÑƒÐ¶ÐµÐ½ Ð±Ð°Ð¹ÐµÑ€', 
    'Ð¸Ñ‰Ñƒ Ð°Ñ€Ð±Ð¸Ñ‚Ñ€Ð°Ð¶Ð½Ð¸ÐºÐ°', 'Ð½ÑƒÐ¶ÐµÐ½ Ð°Ñ€Ð±Ð¸Ñ‚Ñ€Ð°Ð¶Ð½Ð¸Ðº',
    'Ð¸Ñ‰Ñƒ Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ', 'Ð¸Ñ‰ÐµÐ¼ Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ', 'Ð½ÑƒÐ¶ÐµÐ½ Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ',
    'Ð½Ð°Ð±Ð¾Ñ€ Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ', 'Ð½Ð°Ð±Ð¸Ñ€Ð°ÑŽ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ', 'Ð½Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼',
    'Ð¸Ñ‰Ñƒ Ñ„Ð°Ñ€Ð¼ÐµÑ€Ð°', 'Ð½ÑƒÐ¶ÐµÐ½ Ñ„Ð°Ñ€Ð¼ÐµÑ€', 'Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ñ„Ð°Ñ€Ð¼ÐµÑ€',
    'Ð¸Ñ‰Ñƒ ÐºÑ€ÐµÐ°Ñ‚Ð¸Ð²Ñ‰Ð¸ÐºÐ°', 'Ð½ÑƒÐ¶ÐµÐ½ Ð´Ð¸Ð·Ð°Ð¹Ð½ÐµÑ€', 'Ð¸Ñ‰Ñƒ Ð´Ð¸Ð·Ð°Ð¹Ð½ÐµÑ€Ð°',
    'Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ', 'Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸', 'Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð° Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ',
    'Ð¸Ñ‰Ñƒ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°', 'Ð½ÑƒÐ¶ÐµÐ½ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€',
    'Ð¸Ñ‰Ñƒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ°', 'Ð½ÑƒÐ¶ÐµÐ½ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº',
    
    # ÐŸÑ€ÑÐ¼Ð¾Ð¹ Ð½Ð°Ð¹Ð¼ (EN)
    'hiring', 'looking for buyer', 'need media buyer',
    'join our team', 'we are hiring', 'looking for affiliate',
    'seeking traffic', 'need trafficker', 'open position',
    'media buyer', 'sales manager',
    
    # ðŸ‡ºðŸ‡¦ Ð£ÐºÑ€Ð°Ð¸Ð½ÑÐºÐ¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¸ÐºÐ¸
    'ÑˆÑƒÐºÐ°Ñ”Ð¼Ð¾', 'ÑˆÑƒÐºÐ°ÑŽÑ‚ÑŒ', 'ÑˆÑƒÐºÐ°ÑŽ',  # Ð¸Ñ‰ÐµÐ¼/Ð¸Ñ‰ÑƒÑ‚/Ð¸Ñ‰Ñƒ
    'Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±ÐµÐ½', 'Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð°', 'Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ñ–',  # Ð½ÑƒÐ¶ÐµÐ½/Ð½ÑƒÐ¶Ð½Ð°/Ð½ÑƒÐ¶Ð½Ñ‹
    'Ð·Ð°Ð¿Ñ€Ð¾ÑˆÑƒÑ”Ð¼Ð¾', 'Ð·Ð°Ð¿Ñ€Ð¾ÑˆÑƒÑŽ',  # Ð¿Ñ€Ð¸Ð³Ð»Ð°ÑˆÐ°ÐµÐ¼
    'Ð¿Ñ€Ð¸Ñ”Ð´Ð½ÑƒÐ¹ÑÑ', 'Ð¿Ñ€Ð¸Ñ”Ð´Ð½ÑƒÐ¹Ñ‚ÐµÑÑŒ',  # Ð¿Ñ€Ð¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÐ¹ÑÑ
    'Ð¿Ñ€Ð°Ñ†ÑŽÐ²Ð°Ñ‚Ð¸ Ð· Ð½Ð°Ð¼Ð¸', 'Ð´Ð¾ Ð½Ð°Ñ',  # Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ð½Ð°Ð¼Ð¸
    'Ð²Ð°ÐºÐ°Ð½ÑÑ–Ñ', 'Ð²Ð°ÐºÐ°Ð½ÑÑ–Ñ—',  # Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ
    'ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð¿Ñ€Ð¾Ñ„ÐµÑÑ–Ð¾Ð½Ð°Ð»Ñ–Ð²',  # ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»Ð¾Ð²
    'Ñ‚Ð¾Ð±Ñ– Ð´Ð¾ Ð½Ð°Ñ', 'Ñ‚ÐµÐ±Ðµ Ðº Ð½Ð°Ð¼',  # Ñ‚ÐµÐ±Ðµ Ðº Ð½Ð°Ð¼
    'Ñ‡ÐµÐºÐ°Ñ”Ð¼Ð¾', 'Ð¿Ð¸ÑˆÐ¸ @',  # Ð¶Ð´Ñ‘Ð¼, Ð¿Ð¸ÑˆÐ¸ @
    'media buyer', 'Ð¼ÐµÐ´Ñ–Ð° Ð±Ð°Ñ”Ñ€',
    
    # Ð¢Ð¸Ð¼ Ð»Ð¸Ð´ / ÐšÐ¾Ð¼Ð°Ð½Ð´Ð°
    'Ñ‚Ð¸Ð¼Ð»Ð¸Ð´', 'team lead', 'Ñ‚Ð¸Ð¼ Ð»Ð¸Ð´', 'Ð½Ð°ÑˆÐ° ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°',
    'Ñƒ Ð½Ð°Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°', 'Ð¼Ð¾Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°', 'ÐµÑÑ‚ÑŒ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°',
    'Ñ‚Ð¸Ð¼Ð°', 'Ñ‚Ð¸Ð¼ÐºÐ°', 'Ð½Ð°ÑˆÐ° Ñ‚Ð¸Ð¼Ð°',
    'ÑÐ¸Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚Ð¸Ð¼Ð»Ð¸Ð´Ñ‹', 'Ð½Ð°ÑÑ‚Ð°Ð²Ð½Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾',
    
    # ÐŸÑ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ / Ð£ÑÐ»Ð¾Ð²Ð¸Ñ
    'Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÑŽ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ', 'Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°ÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ', 'Ð¾Ñ„Ñ„ÐµÑ€ Ð´Ð»Ñ Ð±Ð°ÐµÑ€Ð°',
    'Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð»Ð°Ñ‚Ð¸Ñ‚ÑŒ', 'Ð¿Ð»Ð°Ñ‚Ð¸Ð¼ Ð¾Ñ‚', 'Ð·Ð¿ Ð¾Ñ‚', 'Ð·/Ð¿',
    'ÑÑ‚Ð°Ð²ÐºÐ°', 'Ñ„Ð¸ÐºÑ +', 'Ñ„Ð¸ÐºÑ+', 'Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð¾Ñ‚ Ð¿Ñ€Ð¾Ñ„Ð¸Ñ‚Ð°', 
    'Ð´Ð¾Ñ…Ð¾Ð´ Ð¾Ñ‚', 'Ð·Ð°Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ðº Ð¾Ñ‚', 'Ð¾Ð¿Ð»Ð°Ñ‚Ð°',
    'Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð¾Ñ…Ð¾Ð´', 'Ð´Ð¾Ñ…Ð¾Ð´ Ð¾Ñ‚ $', 'Ð²Ñ–Ð´ $',
    'ÐºÐ°Ñ€ÑŒÐµÑ€Ð½Ñ‹Ð¹ Ñ€Ð¾ÑÑ‚', 'ÐºÐ°Ñ€\'Ñ”Ñ€Ð½Ð¸Ð¹ Ñ€Ñ–ÑÑ‚',
    'Ð¾Ð¿Ð»Ð°Ñ‡ÑƒÐ²Ð°Ð½Ð° Ð²Ñ–Ð´Ð¿ÑƒÑÑ‚ÐºÐ°', 'Ð¾Ñ‚Ð¿ÑƒÑÐº',
    'Ñ‚Ð¾Ð¿Ð¾Ð²Ñ– ÑƒÐ¼Ð¾Ð²Ð¸', 'Ñ‚Ð¾Ð¿Ð¾Ð²Ñ‹Ðµ ÑƒÑÐ»Ð¾Ð²Ð¸Ñ',
    'Ð³Ð½ÑƒÑ‡ÐºÐ¸Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚', 'Ð²Ñ–Ð´Ð´Ð°Ð»ÐµÐ½Ð¾', 'ÑƒÐ´Ð°Ð»Ñ‘Ð½Ð½Ð¾',
    
    # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ñ‚Ñ€Ð°Ñ„Ð¸ÐºÐ° / Sales
    'Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ñ‚Ñ€Ð°Ñ„Ð¸ÐºÐ°', 'Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð·Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ',
    'ÑÑ…ÐµÐ¼Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð°Ñ„Ð¸Ðº', 'Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð»Ð¸Ð´Ð¾Ð²',
    'Ð¾Ð±Ñ€Ð¾Ð±Ð½Ð¸Ðº Ñ‚Ñ€Ð°Ñ„Ñ–ÐºÑƒ',
    
    # ÐŸÐ°Ñ€Ñ‚Ð½Ñ‘Ñ€ÑÑ‚Ð²Ð¾ Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹
    'Ð¸Ñ‰ÐµÐ¼ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°', 'Ð¸Ñ‰Ñƒ Ð¿Ð°Ñ€Ñ‚Ð½Ñ‘Ñ€Ð°', 'Ð½ÑƒÐ¶ÐµÐ½ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€',
    'ÐºÐ¾Ð»Ð»Ð°Ð±Ð¾Ñ€Ð°Ñ†Ð¸Ñ', 'ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾ÐµÐºÑ‚',
    
    # Ð ÐµÑÑƒÑ€ÑÑ‹ / ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð³Ð¾ Ð¸Ð³Ñ€Ð¾ÐºÐ°
    'Ð°ÐºÐºÐ°ÑƒÐ½Ñ‚Ñ‹ Ð¿Ð¾Ð´ ÐºÐ»ÑŽÑ‡', 'Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ðµ Ð°ÐºÐ¸', 'ÐºÑ€ÐµÐ¾ Ð¿Ð¾Ð´ ÐºÐ»ÑŽÑ‡',
    'Ð·Ð°ÐºÑƒÐ¿Ð°ÐµÐ¼ Ñ‚Ñ€Ð°Ñ„Ð¸Ðº', 'Ð·Ð°ÐºÑƒÐ¿ Ñ‚Ñ€Ð°Ñ„Ð¸ÐºÐ°', 'Ð»ÑŒÑ‘Ð¼ Ð¾Ñ‚',
    'Ð¾Ð±ÑŒÐµÐ¼Ñ‹ Ð¾Ñ‚', 'Ð¾Ð±ÑŠÑ‘Ð¼Ñ‹ Ð¾Ñ‚', 'ÑÐ¿ÐµÐ½Ð´ Ð¾Ñ‚', 'Ð±ÑŽÐ´Ð¶ÐµÑ‚ Ð¾Ñ‚',
    'Ð´Ð°Ñ‘Ð¼ Ð±ÑŽÐ´Ð¶ÐµÑ‚', 'Ð²Ñ‹Ð´ÐµÐ»ÑÐµÐ¼ Ð±ÑŽÐ´Ð¶ÐµÑ‚',
    'Ñ‚Ð¾Ð¿-Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð°Ð¼Ð¸', 'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¼Ð¸ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð°Ð¼Ð¸',
    
    # ÐŸÑ€Ð¸Ð·Ð½Ð°ÐºÐ¸ Ð²Ð»Ð°Ð´ÐµÐ»ÑŒÑ†Ð° ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
    'owner', 'Ð²Ð»Ð°Ð´ÐµÐ»ÐµÑ†', 'Ð¾ÑÐ½Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ', 'founder', 'ceo',
    'Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð¶Ñƒ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð¾Ð¹', 'Ð¼Ð¾Ñ Ð°Ð³ÐµÐ½Ñ†Ð¸Ñ', 'Ð½Ð°ÑˆÐµ Ð°Ð³ÐµÐ½Ñ‚ÑÑ‚Ð²Ð¾',
    
    # HR / Ð ÐµÐºÑ€ÑƒÑ‚Ð¸Ð½Ð³
    'hr_', '@hr', 'Ñ€ÐµÐºÑ€ÑƒÑ‚ÐµÑ€', 'recruiter',
    'Ñ‚Ð¸ Ð½Ð°Ð¼ Ð¿Ñ–Ð´Ñ…Ð¾Ð´Ð¸Ñˆ', 'Ñ‚Ñ‹ Ð½Ð°Ð¼ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸ÑˆÑŒ',
    'Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ð¹ Ð¿Ñ–Ð´ÐºÐ¾Ñ€ÑŽÐ²Ð°Ñ‚Ð¸', 'Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾ÐºÐ¾Ñ€ÑÑ‚ÑŒ'
]

# ÐšÐ¾Ð¼Ð¿Ð¸Ð»Ð¸Ñ€ÑƒÐµÐ¼ regex Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
EMPLOYER_PATTERN = re.compile('|'.join(map(re.escape, EMPLOYER_KEYWORDS)), re.IGNORECASE)


# ÐšÐ°ÑÑ‚Ð¾Ð¼Ð½Ñ‹Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð´Ð°Ñ‚ÐµÐ»ÐµÐ¹
EMPLOYER_SYSTEM_PROMPT = """You are a recruiter scout analyzing Telegram users. Your task is to find EMPLOYERS - people who are HIRING staff for affiliate marketing teams.

ðŸŽ¯ HIGH-VALUE TARGETS (score 8-10):
- Team leads actively looking for media buyers
- Team owners posting job openings
- Agency owners recruiting staff
- People offering salaries, budgets, or profit shares
- Anyone saying "Ð¸Ñ‰Ñƒ Ð±Ð°ÐµÑ€Ð°", "Ð½Ð°Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð² ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ", "hiring"

âœ… MEDIUM TARGETS (score 5-7):
- People mentioning they have a team
- Those discussing staff requirements
- People looking for partners with experience
- Anyone talking about team expansion

âŒ NOT A TARGET (score 1-4):
- Regular affiliates looking for offers
- People looking for work themselves
- Spammers, bots
- Just chatting

ðŸ“ PROVIDE REASON IN BOTH LANGUAGES:
- reason_en: Why this person is hiring (max 80 chars)
- reason_ru: ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ ÑÑ‚Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð´Ð°Ñ‚ÐµÐ»ÑŒ (Ð¼Ð°ÐºÑ 80 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²), Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÐ»ÐµÐ½Ð³: Ñ‚Ð¸Ð¼Ð»Ð¸Ð´, Ñ‚Ð¸Ð¼Ð°, Ð±Ð°Ð¹ÐµÑ€

Respond ONLY with valid JSON:
{
    "is_lead": true/false,
    "score": 1-10,
    "category": "team_lead" | "agency_owner" | "recruiter" | "partner_seeker" | "potential_employer" | "not_employer",
    "reason_en": "Brief explanation IN ENGLISH",
    "reason_ru": "ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ðµ ÐÐ Ð Ð£Ð¡Ð¡ÐšÐžÐœ"
}"""


@dataclass
class EmployerData:
    """Data for potential employer."""
    user_id: int
    username: Optional[str]
    display_name: Optional[str]
    bio: Optional[str] = None
    messages: List[str] = field(default_factory=list)
    matched_keywords: List[str] = field(default_factory=list)
    source_chat: str = ""
    
    def check_employer_keywords(self) -> bool:
        """Check if messages contain employer keywords."""
        text_to_check = ' '.join(self.messages)
        if self.bio:
            text_to_check += ' ' + self.bio
        
        matches = EMPLOYER_PATTERN.findall(text_to_check.lower())
        if matches:
            self.matched_keywords = list(set(matches))[:5]
            return True
        return False


async def scrape_for_employers(
    client,
    chat,
    rate_limiter,
    logger,
    messages_limit: int = 1000
) -> List[EmployerData]:
    """Scrape chat for potential employers."""
    
    chat_title = getattr(chat, 'title', str(chat.id))
    chat_username = getattr(chat, 'username', None)
    
    logger.thought(ThoughtType.SCRAPE, "EmployerScraper", f"ðŸ”Ž Scanning for employers: {chat_title}", {
        "limit": messages_limit
    })
    
    users_data: Dict[int, EmployerData] = {}
    total_messages = 0
    
    try:
        await rate_limiter.wait("message_fetch")
        
        async for message in client.iter_messages(chat, limit=messages_limit):
            if not isinstance(message, Message):
                continue
            
            total_messages += 1
            
            if not message.sender_id:
                continue
            
            sender = message.sender
            if not isinstance(sender, User) or sender.bot:
                continue
            
            text = message.text or message.raw_text or ""
            if len(text.strip()) < 20:  # ÐœÐ¸Ð½Ð¸Ð¼ÑƒÐ¼ 20 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð´Ð»Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹
                continue
            
            user_id = sender.id
            
            if user_id not in users_data:
                display_name = ""
                if sender.first_name:
                    display_name = sender.first_name
                if sender.last_name:
                    display_name += " " + sender.last_name
                
                users_data[user_id] = EmployerData(
                    user_id=user_id,
                    username=sender.username,
                    display_name=display_name.strip() or f"User {user_id}",
                    source_chat=chat_title
                )
            
            users_data[user_id].messages.append(text)
            
            # Progress log
            if total_messages % 200 == 0:
                logger.thought(ThoughtType.SCRAPE, "EmployerScraper", f"Progress: {total_messages} messages", {
                    "users": len(users_data)
                })
            
            # Rate limit delay
            if total_messages % 200 == 0:
                await rate_limiter.batch_delay(200, total_messages)
                
    except FloodWaitError as e:
        logger.thought(ThoughtType.ERROR, "EmployerScraper", f"FloodWait: {e.seconds}s")
        await rate_limiter.handle_flood_wait(e.seconds)
    except Exception as e:
        logger.thought(ThoughtType.ERROR, "EmployerScraper", f"Error scraping: {e}")
    
    # Filter: only users with employer keywords
    employers = []
    for user_data in users_data.values():
        if user_data.check_employer_keywords():
            employers.append(user_data)
    
    # Sort by keyword count
    employers.sort(key=lambda x: len(x.matched_keywords), reverse=True)
    
    logger.thought(ThoughtType.SUCCESS, "EmployerScraper", f"âœ… Found {len(employers)} potential employers in {chat_title}", {
        "total_messages": total_messages,
        "total_users": len(users_data),
        "with_keywords": len(employers)
    })
    
    return employers


async def run_employer_hunt(target_chats: list[str] = None):
    """Main execution flow for finding employers."""
    
    # Initialize
    settings.ensure_directories()
    logger = get_logger(settings.logging.log_file, settings.logging.level)
    
    logger.panel(
        "ðŸ¢ EMPLOYER HUNT MODE",
        "ÐŸÐ¾Ð¸ÑÐº Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð´Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð² Ð°Ñ€Ð±Ð¸Ñ‚Ñ€Ð°Ð¶Ðµ:\n"
        "â€¢ Ð¢Ð¸Ð¼Ð»Ð¸Ð´Ñ‹ Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸ÑÐ¼Ð¸\n"
        "â€¢ Ð’Ð»Ð°Ð´ÐµÐ»ÑŒÑ†Ñ‹ ÐºÐ¾Ð¼Ð°Ð½Ð´\n"
        "â€¢ ÐÐ³ÐµÐ½Ñ‚ÑÑ‚Ð²Ð° Ð½Ð° Ð½Ð°Ð¹Ð¼Ðµ",
        "magenta"
    )
    
    # Validate config
    try:
        settings.validate()
    except ValueError as e:
        logger.error("Configuration", str(e))
        return
    
    # Target chats
    chats_to_scrape = target_chats or settings.scraper.target_chats
    if not chats_to_scrape:
        logger.error("Configuration", "No target chats specified")
        return
    
    logger.thought(ThoughtType.SYSTEM, "EmployerHunt", "Starting Employer Hunt", {
        "target_chats": len(chats_to_scrape),
        "employer_keywords": len(EMPLOYER_KEYWORDS)
    })
    
    # Initialize components
    rate_limiter = RateLimiter(
        min_delay=settings.scraper.delay_min,
        max_delay=settings.scraper.delay_max,
        logger=logger
    )
    
    intent_analyzer = IntentAnalyzer(settings.llm, logger)
    # Override system prompt for employer detection
    from core import intent_analyzer as ia_module
    original_prompt = ia_module.BILINGUAL_SYSTEM_PROMPT
    ia_module.BILINGUAL_SYSTEM_PROMPT = EMPLOYER_SYSTEM_PROMPT
    
    csv_exporter = CSVExporter(settings.export.export_dir, logger)
    
    all_employers: List[EmployerData] = []
    
    # Connect to Telegram
    async with TelegramClient(settings.telegram, logger) as client:
        
        # Phase 1: Join chats
        logger.panel("PHASE 1", "Joining target chats...", "cyan")
        joined_chats = []
        
        for i, link in enumerate(chats_to_scrape):
            try:
                await rate_limiter.wait("join_chat")
                logger.thought(ThoughtType.JOIN_CHAT, "EmployerHunt", f"Joining {i+1}/{len(chats_to_scrape)}: {link}")
                chat = await client.join_chat(link)
                if chat:
                    joined_chats.append(chat)
                    logger.thought(ThoughtType.SUCCESS, "EmployerHunt", f"Joined: {getattr(chat, 'title', link)}")
            except Exception as e:
                logger.thought(ThoughtType.WARNING, "EmployerHunt", f"Failed to join {link}: {e}")
        
        if not joined_chats:
            logger.error("Scraper", "Could not join any chats")
            return
        
        # Phase 2: Scrape for employers
        logger.panel("PHASE 2", "Scanning for employers...", "yellow")
        
        for chat in joined_chats:
            employers = await scrape_for_employers(
                client,
                chat,
                rate_limiter,
                logger,
                settings.scraper.messages_per_chat
            )
            all_employers.extend(employers)
            await rate_limiter.wait("message_fetch")
    
    # Remove duplicates by user_id
    seen_ids: Set[int] = set()
    unique_employers = []
    for emp in all_employers:
        if emp.user_id not in seen_ids:
            seen_ids.add(emp.user_id)
            unique_employers.append(emp)
    
    logger.panel("PHASE 3", f"Analyzing {len(unique_employers)} potential employers with AI...", "green")
    
    # Prepare for AI analysis
    users_data = []
    for emp in unique_employers[:50]:  # Limit to 50 for API costs
        users_data.append({
            "user_id": emp.user_id,
            "username": emp.username,
            "display_name": emp.display_name,
            "bio": emp.bio,
            "messages": emp.messages[:10],  # Top 10 messages
            "source_chat": emp.source_chat,
            "message_count": len(emp.messages),
            "has_keywords": True,
            "matched_keywords": emp.matched_keywords
        })
    
    # AI Analysis
    all_leads = []
    if users_data:
        all_leads = await intent_analyzer.batch_analyze(users_data)
    
    # Restore original prompt
    ia_module.BILINGUAL_SYSTEM_PROMPT = original_prompt
    
    # Phase 4: Export results
    logger.panel("PHASE 4", "Exporting employer leads...", "magenta")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Filter: score >= 5
    employer_leads = [l for l in all_leads if l.is_lead and l.score >= 5]
    
    if employer_leads:
        # Export to CSV
        csv_path = csv_exporter.export_leads(
            employer_leads,
            f"employers_{timestamp}.csv"
        )
        logger.success("Export", f"Employers saved to {csv_path}")
        
        # Export contacts for outreach
        export_dir = Path(settings.export.export_dir)
        
        hot_employers = [l for l in employer_leads if l.score >= 7]
        warm_employers = [l for l in employer_leads if 5 <= l.score < 7]
        
        # Hot employers file
        hot_file = export_dir / f"employers_hot_{timestamp}.txt"
        with open(hot_file, 'w', encoding='utf-8') as f:
            f.write(f"# ðŸ¢ HOT EMPLOYERS (Score >= 7) - {datetime.now().strftime('%d.%m.%Y %H:%M')}\n")
            f.write(f"# Total: {len(hot_employers)} contacts\n")
            f.write("# " + "â”€" * 50 + "\n\n")
            
            for lead in hot_employers:
                if lead.username:
                    f.write(f"@{lead.username}\n")
                else:
                    f.write(f"tg://user?id={lead.user_id}\n")
            
            f.write("\n# " + "â”€" * 50 + "\n")
            f.write("# Details:\n")
            for lead in hot_employers:
                contact = f"@{lead.username}" if lead.username else f"ID:{lead.user_id}"
                f.write(f"# {contact:25} | {lead.score}/10 | {lead.category:18} | {lead.reason_ru[:60]}\n")
        
        # Warm employers file
        warm_file = export_dir / f"employers_warm_{timestamp}.txt"
        with open(warm_file, 'w', encoding='utf-8') as f:
            f.write(f"# ðŸŸ¡ WARM EMPLOYERS (Score 5-6) - {datetime.now().strftime('%d.%m.%Y %H:%M')}\n")
            f.write(f"# Total: {len(warm_employers)} contacts\n\n")
            for lead in warm_employers:
                if lead.username:
                    f.write(f"@{lead.username}\n")
                else:
                    f.write(f"tg://user?id={lead.user_id}\n")
        
        logger.success("Export", f"ðŸ“¤ Hot employers: {hot_file}")
        logger.success("Export", f"ðŸ“¤ Warm employers: {warm_file}")
    
    # Generate HTML Report
    if all_leads:
        report_path = generate_html_report(
            all_leads,
            f"data/report_employers_{timestamp}.html",
            chats_processed=len(joined_chats),
            discovered_links=0
        )
        logger.success("Report", f"HTML report: {report_path}")
        
        # Auto-open
        import webbrowser
        import os
        webbrowser.open('file://' + os.path.realpath(report_path))
    
    # Final summary
    hot_count = len([l for l in employer_leads if l.score >= 7])
    warm_count = len([l for l in employer_leads if 5 <= l.score < 7])
    
    # Category breakdown
    categories = {}
    for lead in all_leads:
        cat = lead.category
        categories[cat] = categories.get(cat, 0) + 1
    
    logger.panel(
        "ðŸ¢ EMPLOYER HUNT Ð—ÐÐ’Ð•Ð Ð¨ÐÐ",
        f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ Ñ‡Ð°Ñ‚Ð¾Ð²: {len(joined_chats)}\n"
        f"ðŸ‘¥ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¸ÐºÐ°Ð¼Ð¸: {len(unique_employers)}\n"
        f"ðŸ¤– ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ AI: {len(all_leads)}\n"
        f"ðŸ”¥ Ð“Ð¾Ñ€ÑÑ‡Ð¸Ñ… Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð´Ð°Ñ‚ÐµÐ»ÐµÐ¹: {hot_count}\n"
        f"ðŸŸ¡ Ð¢Ñ‘Ð¿Ð»Ñ‹Ñ… Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð´Ð°Ñ‚ÐµÐ»ÐµÐ¹: {warm_count}\n"
        f"ðŸ“Š ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸: {categories}\n"
        f"ðŸ’¾ Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð²: {settings.export.export_dir}",
        "green"
    )


def parse_args():
    parser = argparse.ArgumentParser(description="Aura Lead Hunter - Employer Hunt Mode")
    parser.add_argument(
        "--chats",
        type=str,
        help="Comma-separated list of chat usernames to scan"
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    
    target_chats = None
    if args.chats:
        target_chats = [c.strip() for c in args.chats.split(",")]
    
    asyncio.run(run_employer_hunt(target_chats))
